{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Dataset Exploration\n",
    "\n",
    "**Purpose:** Explore v√† analyze table datasets cho Meddies-OCR project\n",
    "\n",
    "**Datasets covered:**\n",
    "1. FinePDFs-Full (24K images, unlabeled)\n",
    "2. PubTables-1M (1M table images, fully annotated)\n",
    "\n",
    "**Tasks:**\n",
    "- Dataset statistics & quality analysis\n",
    "- Sample visualization\n",
    "- Annotation structure exploration\n",
    "- Data preparation for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Setup\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FinePDFs-Full Dataset (Current)\n",
    "\n",
    "Explore the 24K images we already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinePDFs dataset\n",
    "finepdfs_path = Path(\"../data/raw/finepdfs_full/images\")\n",
    "finepdfs_images = sorted(list(finepdfs_path.glob(\"*.jpg\")))\n",
    "\n",
    "print(f\"üìä FinePDFs-Full Statistics:\")\n",
    "print(f\"  Total images: {len(finepdfs_images):,}\")\n",
    "print(f\"  Location: {finepdfs_path}\")\n",
    "\n",
    "# Sample 10 random images\n",
    "sample_indices = random.sample(range(len(finepdfs_images)), 10)\n",
    "print(f\"\\nüé≤ Random samples: {sample_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples in grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_idx in enumerate(sample_indices):\n",
    "    img_path = finepdfs_images[img_idx]\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Image {img_idx:06d}\\n{img.size[0]}x{img.size[1]}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"FinePDFs-Full: Random Samples\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image properties\n",
    "def analyze_image_stats(image_paths, sample_size=1000):\n",
    "    \"\"\"Analyze image dimensions, file sizes, brightness.\"\"\"\n",
    "    sampled = random.sample(image_paths, min(sample_size, len(image_paths)))\n",
    "    \n",
    "    widths, heights, sizes, brightness = [], [], [], []\n",
    "    \n",
    "    for img_path in sampled:\n",
    "        img = Image.open(img_path)\n",
    "        w, h = img.size\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "        sizes.append(os.path.getsize(img_path) / 1024)  # KB\n",
    "        \n",
    "        # Brightness (mean pixel value)\n",
    "        gray = img.convert('L')\n",
    "        brightness.append(np.array(gray).mean())\n",
    "    \n",
    "    return {\n",
    "        'widths': widths,\n",
    "        'heights': heights,\n",
    "        'sizes': sizes,\n",
    "        'brightness': brightness\n",
    "    }\n",
    "\n",
    "print(\"Analyzing 1000 random images...\")\n",
    "stats = analyze_image_stats(finepdfs_images, sample_size=1000)\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].hist(stats['widths'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_title('Width Distribution')\n",
    "axes[0, 0].set_xlabel('Width (pixels)')\n",
    "\n",
    "axes[0, 1].hist(stats['heights'], bins=50, edgecolor='black', color='orange')\n",
    "axes[0, 1].set_title('Height Distribution')\n",
    "axes[0, 1].set_xlabel('Height (pixels)')\n",
    "\n",
    "axes[1, 0].hist(stats['sizes'], bins=50, edgecolor='black', color='green')\n",
    "axes[1, 0].set_title('File Size Distribution')\n",
    "axes[1, 0].set_xlabel('Size (KB)')\n",
    "\n",
    "axes[1, 1].hist(stats['brightness'], bins=50, edgecolor='black', color='red')\n",
    "axes[1, 1].set_title('Brightness Distribution (blank detection)')\n",
    "axes[1, 1].set_xlabel('Mean pixel value (0=black, 255=white)')\n",
    "axes[1, 1].axvline(x=250, color='red', linestyle='--', label='Likely blank (>250)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats\n",
    "print(f\"\\nüìä Image Statistics:\")\n",
    "print(f\"  Width: {np.mean(stats['widths']):.0f} ¬± {np.std(stats['widths']):.0f} pixels\")\n",
    "print(f\"  Height: {np.mean(stats['heights']):.0f} ¬± {np.std(stats['heights']):.0f} pixels\")\n",
    "print(f\"  File size: {np.mean(stats['sizes']):.1f} ¬± {np.std(stats['sizes']):.1f} KB\")\n",
    "print(f\"  Brightness: {np.mean(stats['brightness']):.1f} ¬± {np.std(stats['brightness']):.1f}\")\n",
    "\n",
    "# Estimate blank ratio\n",
    "blank_threshold = 250\n",
    "blank_count = sum(1 for b in stats['brightness'] if b > blank_threshold)\n",
    "blank_ratio = blank_count / len(stats['brightness'])\n",
    "print(f\"\\n‚ö†Ô∏è  Estimated blank ratio: {blank_ratio:.1%} (brightness > {blank_threshold})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PubTables-1M Dataset\n",
    "\n",
    "Explore the annotated table dataset (once download completes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PubTables-1M dataset\n",
    "pubtables_path = Path(\"../data/raw/pubtables-1m\")\n",
    "\n",
    "if not pubtables_path.exists():\n",
    "    print(\"‚è≥ PubTables-1M is still downloading...\")\n",
    "    print(f\"   Check progress: tail -f {pubtables_path}/download.log\")\n",
    "else:\n",
    "    print(f\"‚úÖ PubTables-1M found at {pubtables_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    ds = load_dataset(\n",
    "        \"bsmock/pubtables-1m\",\n",
    "        cache_dir=str(pubtables_path / \".cache\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä PubTables-1M Statistics:\")\n",
    "    for split_name, split_data in ds.items():\n",
    "        print(f\"  {split_name}: {len(split_data):,} samples\")\n",
    "    \n",
    "    print(f\"\\nüìã Features: {list(ds['train'].features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore sample annotations\n",
    "if pubtables_path.exists():\n",
    "    # Get a sample\n",
    "    sample = ds['train'][0]\n",
    "    \n",
    "    print(\"üîç Sample annotation structure:\")\n",
    "    for key, value in sample.items():\n",
    "        if key == 'image':\n",
    "            print(f\"  {key}: PIL.Image {value.size}\")\n",
    "        elif isinstance(value, (list, dict)):\n",
    "            print(f\"  {key}: {type(value).__name__} (len={len(value) if hasattr(value, '__len__') else 'N/A'})\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize annotated table\n",
    "if pubtables_path.exists():\n",
    "    def visualize_table_annotation(sample, figsize=(15, 10)):\n",
    "        \"\"\"Visualize table with bounding boxes and structure.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(sample['image'])\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Image with annotations\n",
    "        axes[1].imshow(sample['image'])\n",
    "        ax = axes[1]\n",
    "        \n",
    "        # Draw bounding boxes (assuming bbox format: [x, y, width, height])\n",
    "        # Note: Adapt this based on actual annotation format\n",
    "        if 'bboxes' in sample or 'objects' in sample:\n",
    "            # This is a placeholder - adjust based on actual format\n",
    "            print(\"Drawing bounding boxes...\")\n",
    "            # Example:\n",
    "            # for bbox in sample['bboxes']:\n",
    "            #     rect = patches.Rectangle(\n",
    "            #         (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "            #         linewidth=2, edgecolor='red', facecolor='none'\n",
    "            #     )\n",
    "            #     ax.add_patch(rect)\n",
    "        \n",
    "        axes[1].set_title('Annotated (with bboxes)')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Visualize 3 random samples\n",
    "    print(\"Visualizing 3 random samples...\")\n",
    "    for i in random.sample(range(100), 3):  # Sample from first 100\n",
    "        sample = ds['train'][i]\n",
    "        visualize_table_annotation(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Comparison\n",
    "\n",
    "Compare FinePDFs vs PubTables for table understanding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Dataset': 'FinePDFs-Full',\n",
    "        'Images': '24,318',\n",
    "        'Size': '21 GB',\n",
    "        'Annotations': '‚ùå None',\n",
    "        'Table-focused': '‚ùå Mixed content',\n",
    "        'Use case': 'Unlabeled pretraining'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'PubTables-1M',\n",
    "        'Images': '1,000,000',\n",
    "        'Size': '~100 GB',\n",
    "        'Annotations': '‚úÖ Full structure',\n",
    "        'Table-focused': '‚úÖ 100% tables',\n",
    "        'Use case': 'Supervised fine-tuning'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation for Training\n",
    "\n",
    "Prepare datasets cho Qwen VL fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert PubTables to Qwen VL format\n",
    "if pubtables_path.exists():\n",
    "    def convert_to_qwen_format(sample):\n",
    "        \"\"\"\n",
    "        Convert PubTables sample to Qwen VL training format.\n",
    "        \n",
    "        Format:\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": <PIL.Image>},\n",
    "                        {\"type\": \"text\", \"text\": \"Extract the table structure\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \"<table>...</table>\"}]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # This is a template - adjust based on actual annotation format\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": sample['image']},\n",
    "                        {\"type\": \"text\", \"text\": \"Extract all text from this table and return as markdown.\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"TODO: Format table structure here\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # Convert sample\n",
    "    sample = ds['train'][0]\n",
    "    qwen_sample = convert_to_qwen_format(sample)\n",
    "    \n",
    "    print(\"üìù Qwen VL format example:\")\n",
    "    print(json.dumps(qwen_sample, indent=2, default=str)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Checks\n",
    "\n",
    "Validate dataset quality before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common issues\n",
    "def quality_check_dataset(dataset, sample_size=100):\n",
    "    \"\"\"Run quality checks on dataset.\"\"\"\n",
    "    issues = {\n",
    "        'corrupted_images': [],\n",
    "        'missing_annotations': [],\n",
    "        'blank_images': [],\n",
    "        'low_resolution': []\n",
    "    }\n",
    "    \n",
    "    samples = random.sample(range(len(dataset)), min(sample_size, len(dataset)))\n",
    "    \n",
    "    for idx in samples:\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        # Check image\n",
    "        try:\n",
    "            img = sample['image']\n",
    "            w, h = img.size\n",
    "            \n",
    "            # Low resolution\n",
    "            if w < 800 or h < 600:\n",
    "                issues['low_resolution'].append(idx)\n",
    "            \n",
    "            # Blank detection\n",
    "            gray = img.convert('L')\n",
    "            brightness = np.array(gray).mean()\n",
    "            if brightness > 250:\n",
    "                issues['blank_images'].append(idx)\n",
    "                \n",
    "        except Exception as e:\n",
    "            issues['corrupted_images'].append((idx, str(e)))\n",
    "        \n",
    "        # Check annotations (adapt based on format)\n",
    "        # if not sample.get('bboxes'):\n",
    "        #     issues['missing_annotations'].append(idx)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "if pubtables_path.exists():\n",
    "    print(\"Running quality checks on PubTables-1M...\")\n",
    "    issues = quality_check_dataset(ds['train'], sample_size=100)\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  Quality Check Results:\")\n",
    "    for issue_type, issue_list in issues.items():\n",
    "        if issue_list:\n",
    "            print(f\"  {issue_type}: {len(issue_list)} found\")\n",
    "        else:\n",
    "            print(f\"  {issue_type}: ‚úÖ None found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Filter blank images** from FinePDFs (current estimate: ~X% blank)\n",
    "2. **Setup training pipeline** with PubTables-1M:\n",
    "   - Start with small subset (1K samples) for quick iteration\n",
    "   - Scale to full dataset once pipeline works\n",
    "3. **Fine-tune Qwen VL** for table understanding:\n",
    "   - Task: Table structure extraction\n",
    "   - Output format: Markdown or HTML tables\n",
    "4. **Evaluate** on held-out test set:\n",
    "   - Structure accuracy (row/column correctness)\n",
    "   - Cell-level CER (Character Error Rate)\n",
    "   - End-to-end table extraction accuracy\n",
    "\n",
    "**Reference CLAUDE.md:**\n",
    "- Section 1: VLM Architecture Fundamentals\n",
    "- Section 2.1: Qwen 2.5 VL (Primary cho OCR tasks)\n",
    "- Section 5: Common Tasks & Commands"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
